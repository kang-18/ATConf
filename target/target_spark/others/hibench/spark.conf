# Spark home
hibench.spark.home      /home/wk/sd_spark/spark/spark-bodropout-test/spark

# Spark master
#   standalone mode: spark://xxx:7077
#   YARN mode: yarn-client
hibench.spark.master    spark://192.168.1.154:7077
 

# executor number and cores when running on Yarn
# hibench.yarn.executor.num       
# hibench.yarn.executor.cores     

spark.driver.extraJavaOptions      -XX:NewRatio={{app_config.jvm_new_ratio}} -XX:SurvivorRatio={{app_config.jvm_survivor_ratio}} -XX:MaxGCPauseMillis={{app_config.jvm_max_gc_pause}} -XX:ParallelGCThreads={{app_config.jvm_par_gc_thread}} -XX:{{app_config.jvm_gc_collect}} -XX:GCTimeRatio={{app_config.jvm_gc_timeratio}} -XX:MaxTenuringThreshold={{app_config.jvm_max_tenuringThreshold}}
spark.executor.extraJavaOptions    -XX:NewRatio={{app_config.jvm_new_ratio}} -XX:SurvivorRatio={{app_config.jvm_survivor_ratio}} -XX:MaxGCPauseMillis={{app_config.jvm_max_gc_pause}} -XX:ParallelGCThreads={{app_config.jvm_par_gc_thread}} -XX:{{app_config.jvm_gc_collect}} -XX:GCTimeRatio={{app_config.jvm_gc_timeratio}} -XX:MaxTenuringThreshold={{app_config.jvm_max_tenuringThreshold}}


# executor and driver memory in standalone & YARN mode
spark.driver.cores                          {{app_config.spark_driver_cores}}
spark.driver.maxResultSize                  {{app_config.spark_driver_maxResultSize}}g
spark.driver.memory                         {{app_config.spark_driver_memory}}m
spark.executor.memory                       {{app_config.spark_executor_memory}}m

spark.reducer.maxSizeInFlight               {{app_config.spark_reducer_maxSizeInFlight}}m
spark.shuffle.compress                      {{app_config.spark_shuffle_compress}}
spark.shuffle.file.buffer                   {{app_config.spark_shuffle_file_buffer}}k
spark.shuffle.service.index.cache.entries   {{app_config.spark_shuffle_service_index_cache_entries}}
spark.shuffle.sort.bypassMergeThreshold     {{app_config.spark_shuffle_sort_bypassMergeThreshold}}
spark.shuffle.spill.compress                {{app_config.spark_shuffle_spill_compress}}
spark.shuffle.accurateBlockThreshold        {{app_config.spark_shuffle_accurateBlockThreshold}}

spark.broadcast.compress                    {{app_config.spark_broadcast_compress}}
spark.io.compression.codec                  {{app_config.spark_io_compression_codec}}
spark.io.compression.snappy.blockSize       {{app_config.spark_io_compression_blockSize}}k
spark.io.compression.lz4.blockSize          {{app_config.spark_io_compression_blockSize}}k
spark.kryo.referenceTracking                {{app_config.spark_kryo_referenceTracking}}
spark.kryoserializer.buffer.max             {{app_config.spark_kryoserializer_buffer_max}}m
spark.kryoserializer.buffer                 {{app_config.spark_kryoserializer_buffer}}k
spark.rdd.compress                          {{app_config.spark_rdd_compress}}
spark.serializer                            {{app_config.spark_serializer}}


spark.memory.fraction                       {{app_config.spark_memory_fraction}}
spark.memory.storageFraction                {{app_config.spark_memory_storageFraction}}
spark.memory.offHeap.enabled                {{app_config.spark_memory_offHeap_enabled}}
spark.memory.offHeap.size                   {{app_config.spark_memory_offHeap_size}}m

spark.broadcast.blockSize                   {{app_config.spark_broadcast_blockSize}}m
spark.executor.cores                        {{app_config.spark_executor_cores}}
spark.executor.heartbeatInterval            {{app_config.spark_executor_heartbeatInterval}}s
spark.storage.memoryMapThreshold            {{app_config.spark_storage_memoryMapThreshold}}m

spark.network.timeout	                      {{app_config.spark_network_timeout}}s

spark.locality.wait                         {{app_config.spark_locality_wait}}s
spark.scheduler.revive.interval             {{app_config.spark_scheduler_revive_interval}}s
spark.speculation                           {{app_config.spark_speculation}}
spark.speculation.interval                  {{app_config.spark_speculation_interval}}ms
spark.speculation.multiplier                {{app_config.spark_speculation_multiplier}}
spark.speculation.quantile                  {{app_config.spark_speculation_quantile}}
spark.task.maxFailures                      {{app_config.spark_task_maxFailures}}


spark.default.parallelism                   {{app_config.spark_default_parallelism}}

spark.sql.shuffle.partitions                {{app_config.spark_sql_shuffle_partitions}}



spark.yarn.jars                             hdfs://192.168.1.154:9000/spark-yarn/jars/*





#======================================================
# Spark Streaming
#======================================================
# Spark streaming Batchnterval in millisecond (default 100)
hibench.streambench.spark.batchInterval          500

# Number of nodes that will receive kafka input (default: 4)
hibench.streambench.spark.receiverNumber        4

# Indicate RDD storage level. (default: 2)
# 0 = StorageLevel.MEMORY_ONLY
# 1 = StorageLevel.MEMORY_AND_DISK_SER
# other = StorageLevel.MEMORY_AND_DISK_SER_2
hibench.streambench.spark.storageLevel 2

# indicate whether to test the write ahead log new feature (default: false)
hibench.streambench.spark.enableWAL false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty (default: /var/tmp)
hibench.streambench.spark.checkpointPath /var/tmp

# whether to use direct approach or not (dafault: true)
hibench.streambench.spark.useDirectMode true
